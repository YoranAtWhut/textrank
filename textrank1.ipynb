{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用textrank来求得关键句\n",
    "## 一、TextRank算法介绍\n",
    "### 1.1 PageRank算法\n",
    "PageRank是根据网页之间相互的超链接计算网页重要性 的技术，是Google的创始人拉里·佩奇和谢尔盖·布林于1998年在斯坦福大学发明了这项技术。\n",
    "\n",
    "提到重要性 我们就会联想到我们上一个实验也计算了每一个单词的重要性，但是在PageRank算法中是为每个页面计算重要性 ，而TextRank对PageRank算法做了改进，使其可以计算每一个句子的重要性 。\n",
    "我们通过一个例子来理解PageRank：\n",
    "![TextRank](./pagerank.png)\n",
    "\n",
    "我们可以看到这个图中有不同颜色的球，代表的是不同的页面，而图中的箭头代表的是 超链接 。如图中D球指向A球，代表的就是D界面有指向A界面的超链接，相当于是D界面“推荐”了A界面。PageRank就是基于“推荐”的算法。\n",
    "\n",
    "我们看到各个界面，既有推荐出去 (Out) 的箭头，也有别的界面推荐进来 (In) 的箭头。于是我们就可以列出一张邻接矩阵来描述整张图(忽略紫色的球！)。\n",
    "\n",
    "0代表没有推荐，1代表的是有推荐。\n",
    "\n",
    "每一列代表的是你推荐的页面 (Out)。如A列，代表的就是A推荐的页面。如表格所描述的，A谁也没有推荐。\n",
    "\n",
    "每一行代表的是谁推荐了你 (In)。如A行，代表的就是推荐A的页面。如表格所描述的，D推荐了A。\n",
    "\n",
    "in\\out | A | B | C | D | E | F |\n",
    "-------|---|---|---|---|---|---|\n",
    "A|0|0|0|1|0|0|\n",
    "B|0|0|1|1|1|1|\n",
    "C|0|1|0|0|0|0|\n",
    "D|0|0|0|0|1|0|\n",
    "E|0|0|0|0|0|1|\n",
    "F|0|0|0|0|1|0|\n",
    "\n",
    "有了这张邻接矩阵作为基础，我们就能够计算PageRank的核心：每一个页面的分数S。\n",
    "![TextRank](./equation.png)\n",
    "Vi 代表的是每一个顶点，及页面。\n",
    "\n",
    "d 代表的是一个阻尼常数 (0<d<1)，用来确保每一个页面都至少有 (1-d)的分数。\n",
    "\n",
    "In(Vi) 代表的是推荐Vi的页面。\n",
    "\n",
    "Out(Vi) 代表的是Vi推荐的页面。\n",
    "\n",
    "|Out(Vi)| 代表的是Vi推荐页面的数量。\n",
    "\n",
    "我们会发现，每一个页面的分数 S( Vi ) 都是依赖于别的页面的分数 S( Vj ) 的。我们需要对每个页面的分数进行初始化。初始化的数字并不重要，因为通过数学可以证明初始化的值对于最终的结果并不影响，只不过会影响算法迭代的次数。我们这里为了计算方便，把初始的 S 都设为 1 。 d 设为 0.85 (参考 S. Brin, L Page 1998年的论文)\n",
    "\n",
    "就这样一步一步算出每一个顶点的值，然后接着从A页面开始继续算，一遍一遍迭代，直到每个页面的分数都不再变化为止。这样，我们就得到了每一个页面的评分 S\n",
    "\n",
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 TextRank算法\n",
    "当我们把PageRank应用到我们的文本当中去的时候，我们首先会发现的问题是，句子和句子之间 如何相互“推荐”?\n",
    "TextRank给出了相应的算法。\n",
    "![TextRank](./textrank.png)\n",
    "\n",
    "    注意！这里 S 代表的不是PageRank中的分数，它代表的是 Sentence 代表的是每一个句子。\n",
    "\n",
    "    Si 代表的是第 i 个句子。\n",
    "\n",
    "    wk 代表的是句子中第 k 个单词。\n",
    "\n",
    "    |Si| 代表的是句子中单词的个数。\n",
    "\n",
    "    { wk| wk ∈ Si & wk ∈ Sj } 代表着同时在 Si 和 Sj 中出现的单词。\n",
    "\n",
    "根据这个就可以求出两个句子之间的相似度，也就是推荐程度。\n",
    "\n",
    "举个例子。\n",
    "\n",
    "    I am a fool. A big fool. I like fool.\n",
    "\n",
    "    前两句句子当中都有的单词是 a 和 fool，两个单词。\n",
    "\n",
    "    第一句话和第三句话都有的单词是 I 和 fool。\n",
    "\n",
    "    后两句句子当中都有的单词是 fool .\n",
    "\n",
    "    第一句话长度是 4，第二句话长度是3, 第三句话长度是3.\n",
    "    \n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "Similarity(S1,S2) = 2 / ( log(3) + log(4) ) = 0.80\n",
    "\n",
    "Similarity(S1,S3) = 2 / ( log(3) + log(3) ) = 0.91\n",
    "\n",
    "Similarity(S2,S3) = 1 / ( log(3) + log(4) ) = 0.40\n",
    "\n",
    "注意！两个句子的相似度没有指向性. 也就是说Similarity(S1,S2) = Similarity(S2,S1)\n",
    "相似度不再只有0 和 1 ，它是浮点数的集合。\n",
    "\n",
    "相似度 | 第一句 | 第二句 | 第三句 |\n",
    "------|-------|-------|-------|\n",
    "第一句|none|0.80|0.91|\n",
    "第二句|0.80|none|0.40|\n",
    "第三句|0.91|0.40|none|\n",
    "\n",
    "我们写出来类似PageRank中的邻接矩阵。\n",
    "在此基础上，我们可以应用新的PageRank算法来完成分数的计算\n",
    "![TextRank](./textequation.png)\n",
    "\n",
    "WS(Vi) 代表的是Vi这个页面的分数\n",
    "\n",
    "d 代表的是一个阻尼常数 (0<d<1)，用来确保每一个页面都至少有 (1-d)的分数。\n",
    "\n",
    "In(Vi) 代表的是推荐Vi的页面。\n",
    "\n",
    "Out(Vi) 代表的是Vi推荐的页面。\n",
    "\n",
    "wji 代表的是 Vi 和 Vj 之间的相似度。\n",
    "\n",
    "我们拿第一句句子作为例子，看一看它的得分。同样的，初始分数都是 1 。\n",
    "\n",
    "WS(V1) = (1 - 0.85) + 0.85 * \n",
    "( /*第二句句子*/ (0.80 * 1) / (0.80 + 0.40) + \n",
    "  /*第三句句子*/ (0.91 * 1) / (0.91 + 0.40) ) = 1.30\n",
    "  \n",
    "至此我们进一步迭代，计算出第二句和第三句的 分数 ，然后继续从第一句开始计算。直到最终每一句的分数不再变化为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 二、实验步骤\n",
    "### 2.1准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from itertools import product,count\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english')+list(punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2、计算相似性\n",
    "这个函数计算两个句子之间的相似性。有关相似性的知识，请看上文的算法介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "传入两个句子，\n",
    "返回这两个句子相似度。\n",
    "'''\n",
    "def calculate_similarity(sen1,sen2):\n",
    "    # 设置counter计数器\n",
    "    counter = 0\n",
    "    for word in sen1:\n",
    "        if word in sen2:\n",
    "            counter += 1\n",
    "    return counter/(math.log(len(sen1))+math.log(len(sen2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3创建相似度邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "传入句子的列表\n",
    "返回各个句子之间相似度的图\n",
    "（邻接矩阵表示）\n",
    "\"\"\"\n",
    "def create_graph(word_sent):\n",
    "    num = len(word_sent)\n",
    "    # 初始化表\n",
    "    board = [[0.0 for _ in range(num)] for _ in range(num)]\n",
    "    \n",
    "    for i,j in product(range(num),repeat=2):\n",
    "        if i != j:\n",
    "            board[i][j] = calculate_similarity(word_sent[i],word_sent[j])\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4、根据PAGERANK算法，算出句子分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "输入相似度邻接矩阵\n",
    "返回各个句子的分数\n",
    "\"\"\"\n",
    "def weighted_pagerank(weight_graph):\n",
    "    # 把初始化的分数值设置为0.5\n",
    "    scores = [0.5 for _ in range(len(weight_graph))]\n",
    "    old_scores = [0.0 for _ in range(len(weight_graph))]\n",
    "    \n",
    "    # 开始迭代\n",
    "    while different(scores,old_scores):\n",
    "        for i in range(len(weight_graph)):\n",
    "            old_scores[i] = scores[i]\n",
    "        for i in range(len(weight_graph)):\n",
    "            scores[i] = calculate_score(weight_graph,scores,i)\n",
    "    return scores\n",
    "\n",
    "def different(scores,old_scores):\n",
    "    flag = False\n",
    "    for i in range(len(scores)):\n",
    "        if math.fabs(scores[i]-old_scores[i]) >= 0.0001:\n",
    "            flag = True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "'''\n",
    "根据公式求出指定句子的分数\n",
    "'''\n",
    "def calculate_score(weight_graph,scores,i):\n",
    "    length = len(weight_graph)\n",
    "    d = 0.85\n",
    "    added_score = 0.0\n",
    "    \n",
    "    for j in range(length):\n",
    "        fraction = 0.0\n",
    "        denominator = 0.0\n",
    "        # 先计算分子\n",
    "        fraction = weight_graph[j][i] * scores[j]\n",
    "        # 计算分母\n",
    "        for k in range(length):\n",
    "            denominator += weight_graph[j][k]\n",
    "        added_score += fraction/denominator\n",
    "    # 算出最终的分数\n",
    "    weighted_scores = (1-d)+d*added_score\n",
    "    \n",
    "    return weighted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5、找出分数最高的两个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(text,n):\n",
    "    # 首先分出句子\n",
    "    sents = sent_tokenize(text)\n",
    "    # 然后分出单词\n",
    "    # word_sent是一个二维的列表\n",
    "    # word_sent[i]代表的是第i句\n",
    "    # word_sent[i][j]代表的是\n",
    "    # 第i句中的第j个单词\n",
    "    word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "    \n",
    "    # 把停用词去除\n",
    "    for i in range(len(word_sent)):\n",
    "        for word in word_sent[i]:\n",
    "            if word in stopwords:\n",
    "                word_sent[i].remove(word)\n",
    "    similarity_graph = create_graph(word_sent)\n",
    "    scores = weighted_pagerank(similarity_graph)\n",
    "    sent_selected = nlargest(n,zip(scores,count()))\n",
    "    sent_index = []\n",
    "    for i in range(n):\n",
    "        sent_index.append(sent_selected[i][1])\n",
    "    return [sents[i] for i in sent_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The PHE website and app has a quiz that gives users a health score based on their lifestyle habits by asking questions such as, \"Which snacks do you eat in a normal day?\"', 'The campaign\\'s clinical adviser, Prof Muir Gray, said it was about trying to make people have a different attitude to an \"environmental problem\".']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('news.txt','r') as myfile:\n",
    "        text = myfile.read().replace('\\n','')\n",
    "        print(summarize(text,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
